# AWS FUndementals 
#### Types of Data
1. Structured Data - -easily queriable,organized columns,cleanup already(psql,oracle,csv and execel if there area structured)
2. Unstructured Data - doesnt have a predefined schema,raw text files,videos,audio,emails
3. Semi Structured data - xml and jsons

#### Properties of Data 
1. Volume - size of data, 
2. Velocity - speed at which new data is generated collected procossesd(batch vs streaming)
3. variety  - types of data, strututed vs semi ,source of data ,format of data

### Data Warehouse vs Data Lake 

#### Data warehouse (Extract Trsanform and then load to Data warehouse)
A centralized repository optimized for analysis where data from different sources is stored in strutured format 
designed for analysis 
optimized for read heavy operations 

Examples : Amazon Redshift,Google Bigquery ,Azure SQL Data warehouse 

#### Data Lake (Extract Load and then transform when we need to )
do not preprocess just store all the data in lake house 

Example : Amazon s3,HDFS

Which to choose  : 
data warehouse : when you have strtured data and want to use ti for analytics 
data lakes : need raw data for machine learning 


### Data Lakehouse : Combingin Datawarehouse and Datalake

# Data Lineage
### Spline Agent
it store it in graph data base called Amazon Neptune


Schema Evolution:

Schema GLue REgistry has backward compability 
### Sage Maker


Data base optimizations : 
Adding indexes 
Paritioning 
Compression : use better file formats ,also compression (GZIP,LZOP also talk about these)

Data Sampling techniqies
1. Random Sampling 
2. Stratified Sampling 
3. Systemic Sampling (every thrid record )

Data Skewing 

temporal skew old data has less data and new data bassed on same partition could be huge

How do you partition :

1. Adaptive Partitioning 
2. Salting 
3. Regular Repartition (not recommended)
4. Sampling 
5. Custom parititiong 

## AWs S3
1. s3 bucket names are unique across globe 
Naming Conventions : 
no uppercase no underscores no ip must start with lower case  must not start with xn must not end with s3 s3alias
key is basciclly the full path of the file without the s3b bucekt name 
example : s3:<<bucketname>>/folder1/subfolder1/myfile.txt
here the key is /folder1/subfolder1/myfile.txt
value is the content of the file
max size is 5TB 
if each file is more than 5gb then use multi-part upload to upload files


Amazon s3 Security 
1. User based 
iam policies : which specific user should be allowed from IAM 
2. Resource Based :
bucket wide policies 
Object ACLs(Access Control Lists)
Bucket ACLs()
3. Encrpty objects on s3 using encryption keys 

S3 JSON Example :
there is this policy genertor in aws console we can use to generate policies


### S3 versioning 

Use to protect from unintended deletes and also we can roll back to a version 


this is on a bucket level

when ever we delete files on versionsing basially it just adds delete market to the file 

how to restore files of deleted files is you delete the delete marker

### S3 Replication 
also have to enable versioning in both source and target buckets
By default delete markers are not replicated by this can be changed in replication rule
also if you delete a source object it is not replicated in the target bucket 

has two things CRR(Cross region replication) and SRR(Same Region Replication)

### S3 Storage Classes
1. Amazon S3 Standard - General Purpose 
2. Amazon S3 Standard - Infrequent Access(IA)
3. Amazon S3 One Zone - Infrequent Access
4. Amazon S3 Glacier Instant Retrival 
5. Amazon S3 Glacier Flexible Retrival 
6. Amazon S3 Glacier Deep dive 
7. Amazon S3 Intelligent Tiering

Mainly depends on two things Durability and Availability


Durability : Highly Durable (99.9(11 times))
Availablility -  depends on storage classe

<<add print screen of s3 durablity and availability classe>>



## S3 Storage 

### Dynamo DB 
No SQL Databases 
Scale horizontally can have more instances 
Millions of requests per second
each item can have 400kb of data 


other examples mongodb 



Primary keys 
just partition key 
In dynamodb partition key must be unique to each item

partition key and sort key make the primary key 
Example : (User_id,game_id) PK,Score ,Result

Creating Dynamo Db table :

Select parition key 
select table class (DynamoDb STandard or DynamoDB Standard IA(Infrequently Accessed))

Two modes here 
Read and Write Capacity :
Provisioned Mode(Default)
On DEmand Mode 
expensive than provisioned mod
read/writes automatically scale

Burst Capacity temorarily increase read and write capacity 
if the burst capacity exceeds then "provisionedthroughputexceededExecption" will come then we have to retry this is called "exponential backoff"

Number of WCU needed = number of times we write per second * CEIL(itemsize)/1KB
here 1 kb is the limit of write

Strongly Consistent Read and Evenntually Consistent Read:
1 RCU represents one strongly consistent read or two Eventually consistent read per second for an item of 4 KB
Number of RCU needed = Strongly Consistent Read * (size /4)
or 
Eventlually Consistent /2 * (size/4)
Eventually Consistent read is replica dindnt happen yet so we may get stale data

always round up to nearest 4kB
Strongly Consistent read(consues twice the RCU) we dont get state data and we dont get stale data

on demand is 2.5 times more expensive than provisioned

### Dynamo Db Paritions 


### Dynamo DB API Calls 

PutItem : create or update all attributes
UpdateItem : update attributes 
ConditionalWrites: 
GetItem: projection expression retirve only few attributes 

Query :
Conditional expression : >=,<=,between,begins
Filter Expression : use only with non key attributes

max of 1MB and for bigger number of items use pagination

DeleteItem: based on filter conditions
DeleteTable:

BatchWriteItem:
upto 25 putItem or delete in one call
upto 16MB written or upto 400kb per item
canntor updateItem

BatchGetItem:
upto 100items or 16MB of Data
 if failed use exponentialbackoff or retry or increase RCU
projection is selecting only few columns on the index

#### Dynamo DB PartiQL
Cannot do joins 
#### DYnamo DB Indexes :

LSI(Local Secondary Index )
basically like an additional sort key 
must be defnied at table creation time 
upto 5 LSI

GSI(GLobal Secondary Index)
added or modified after table has been created
speeds up query on non key attributes

if writes are throttled on GSI then main table will be throttiling
for LSI main table RCus will be use so 

default TTL(Time To Live) is 5 minutes 
##EleasticCache 
is used to store aggregated data instead of querying the data everytime


