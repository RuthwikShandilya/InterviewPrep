# AWS FUndementals 
#### Types of Data
1. Structured Data - -easily queriable,organized columns,cleanup already(psql,oracle,csv and execel if there area structured)
2. Unstructured Data - doesnt have a predefined schema,raw text files,videos,audio,emails
3. Semi Structured data - xml and jsons

#### Properties of Data 
1. Volume - size of data, 
2. Velocity - speed at which new data is generated collected procossesd(batch vs streaming)
3. variety  - types of data, strututed vs semi ,source of data ,format of data

### Data Warehouse vs Data Lake 

#### Data warehouse (Extract Trsanform and then load to Data warehouse)
A centralized repository optimized for analysis where data from different sources is stored in strutured format 
designed for analysis 
optimized for read heavy operations 

Examples : Amazon Redshift,Google Bigquery ,Azure SQL Data warehouse 

#### Data Lake (Extract Load and then transform when we need to )
do not preprocess just store all the data in lake house 

Example : Amazon s3,HDFS

Which to choose  : 
data warehouse : when you have strtured data and want to use ti for analytics 
data lakes : need raw data for machine learning 


### Data Lakehouse : Combingin Datawarehouse and Datalake

# Data Lineage
### Spline Agent
it store it in graph data base called Amazon Neptune


Schema Evolution:

Schema GLue REgistry has backward compability 
### Sage Maker


Data base optimizations : 
Adding indexes 
Paritioning 
Compression : use better file formats ,also compression (GZIP,LZOP also talk about these)

Data Sampling techniqies
1. Random Sampling 
2. Stratified Sampling 
3. Systemic Sampling (every thrid record )

Data Skewing 

temporal skew old data has less data and new data bassed on same partition could be huge

How do you partition :

1. Adaptive Partitioning 
2. Salting 
3. Regular Repartition (not recommended)
4. Sampling 
5. Custom parititiong 

## AWs S3
1. s3 bucket names are unique across globe 
Naming Conventions : 
no uppercase no underscores no ip must start with lower case  must not start with xn must not end with s3 s3alias
key is basciclly the full path of the file without the s3b bucekt name 
example : s3:<<bucketname>>/folder1/subfolder1/myfile.txt
here the key is /folder1/subfolder1/myfile.txt
value is the content of the file
max size is 5TB 
if each file is more than 5gb then use multi-part upload to upload files


Amazon s3 Security 
1. User based 
iam policies : which specific user should be allowed from IAM 
2. Resource Based :
bucket wide policies 
Object ACLs(Access Control Lists)
Bucket ACLs()
3. Encrpty objects on s3 using encryption keys 

S3 JSON Example :
there is this policy genertor in aws console we can use to generate policies


### S3 versioning 

Use to protect from unintended deletes and also we can roll back to a version 


this is on a bucket level

when ever we delete files on versionsing basially it just adds delete market to the file 

how to restore files of deleted files is you delete the delete marker

### S3 Replication 
also have to enable versioning in both source and target buckets
By default delete markers are not replicated by this can be changed in replication rule
also if you delete a source object it is not replicated in the target bucket 

has two things CRR(Cross region replication) and SRR(Same Region Replication)

### S3 Storage Classes
1. Amazon S3 Standard - General Purpose 
2. Amazon S3 Standard - Infrequent Access(IA)
3. Amazon S3 One Zone - Infrequent Access
4. Amazon S3 Glacier Instant Retrival 
5. Amazon S3 Glacier Flexible Retrival 
6. Amazon S3 Glacier Deep dive 
7. Amazon S3 Intelligent Tiering

Mainly depends on two things Durability and Availability


Durability : Highly Durable (99.9(11 times))
Availablility -  depends on storage classe

<<add print screen of s3 durablity and availability classe>>



## S3 Storage 

### Dynamo DB 
No SQL Databases 
Scale horizontally can have more instances 
Millions of requests per second
each item can have 400kb of data 


other examples mongodb 



Primary keys 
just partition key 
In dynamodb partition key must be unique to each item

partition key and sort key make the primary key 
Example : (User_id,game_id) PK,Score ,Result

Creating Dynamo Db table :

Select parition key 
select table class (DynamoDb STandard or DynamoDB Standard IA(Infrequently Accessed))

Two capacity modesmodes here 
Read and Write Capacity :
Provisioned Mode(Default)
On DEmand Mode 
expensive than provisioned mod
read/writes automatically scale

Burst Capacity temorarily increase read and write capacity 
if the burst capacity exceeds then "provisionedthroughputexceededExecption" will come then we have to retry this is called "exponential backoff"

Number of WCU needed = number of times we write per second * CEIL(itemsize)/1KB
here 1 kb is the limit of write

Strongly Consistent Read and Evenntually Consistent Read:
1 RCU represents one strongly consistent read or two Eventually consistent read per second for an item of 4 KB
Number of RCU needed = Strongly Consistent Read * (size /4)
or 
Eventlually Consistent /2 * (size/4)
Eventually Consistent read is replica dindnt happen yet so we may get stale data

always round up to nearest 4kB
Strongly Consistent read(consues twice the RCU) we dont get state data and we dont get stale data

on demand is 2.5 times more expensive than provisioned

### Dynamo Db Paritions 


### Dynamo DB API Calls 

PutItem : create or update all attributes
UpdateItem : update attributes 
ConditionalWrites: 
GetItem: projection expression retirve only few attributes 

Query :
Conditional expression : >=,<=,between,begins
Filter Expression : use only with non key attributes

max of 1MB and for bigger number of items use pagination

DeleteItem: based on filter conditions
DeleteTable:

BatchWriteItem:
upto 25 putItem or delete in one call
upto 16MB written or upto 400kb per item
canntor updateItem

BatchGetItem:
upto 100items or 16MB of Data
 if failed use exponentialbackoff or retry or increase RCU
projection is selecting only few columns on the index

#### Dynamo DB PartiQL
Cannot do joins 
#### DYnamo DB Indexes :

LSI(Local Secondary Index )
basically like an additional sort key 
must be defnied at table creation time 
upto 5 LSI

GSI(GLobal Secondary Index)
added or modified after table has been created
speeds up query on non key attributes

if writes are throttled on GSI then main table will be throttiling
for LSI main table RCus will be use so 

default TTL(Time To Live) is 5 minutes 
##EleasticCache 
is used to store aggregated data instead of querying the data everytime

Dynamo DB Security and Other features:
Migration can migrate oracle and sql data into dynamo db

Amazon RDS : Not for Big data but for normal workloads

Amazon Aurora : for mqsql and postgres:
is 5x faster than normal mysql dbs and 3x faster than postgres
1/10 cost of dbs
128TB of db volume
15 read replicas
constinuots back up to s3
aurora serverless for auto scaling

Shared and exclusive locks
Shared lock is lock on write but reads happen
exclusive is lock on read and write on the table 
Shared Lock
example : select * from table where department ='Finance' FOR SHARE;

Exclusive Lock : 
Select * from table where department ='Finance' FOR UPDATE;

RDS Best practices :

1. Use cloud watch to monitor CPU.memory 
2. Perform automatic backups when the write on the table are less
3. Rate limits on Amazon gateway
4. Use proper Indexes
5. Avoid full data scans
6. Analyze table command
7. 

#Amazon DocumentDB(cloud version of mongodb): used to store JSON data 

## Amzon Memroy DB for REDIS :
ultra fast performance with 160millions requests /second
can scale storage from 10Gb to 100TBS

amazon Neptune :
Graph database example social network (posts likes,comments everthing connected in a graph)
query languges for Nepture: Gremline,openCypher,SparQL

Amazon Timestream:

Fully managed fast scalable timeseries database
trillions of time events 
scaling,replication SQL compatible 
Use cases: IOT applications 


Amazon Redshift :
all analytical 
AWS Distributed petabyte scale data warehouse
designed for OLAP not OLTP
Massive parallel processing (MPP),columnar storage
scaling up and down 
replication and backups
monitoring cloudwatch

Architectire : 

Leader nodes : query plans,communicates between comute nodes 
Compute Nodes : executes the plan , 
node slices : each compute node has node slices that processees chunsk of data 


Redshift Spectrum :
queries exa bytes of unstrcutured data in s3 without loading 
seperates storage and compute
gzip and snappy compression


Redshift Durablility
backsup to s3 automatically asynchronoulsy replicated to other regions
failed nodes are automaticalluy replaced

Redshift Distibibution : 
1. AUTO

2. EVEN 
leader nodes dirstibutes in round robin pattern
3. KEY
based on values on one column 
4. ALL
a copy of entire table is sent to all compute nodes

Importing / Exporting data 
COPY Command 
to load to reshift 

UNLOAD command to unload data into files to S3
Amazon Aurora zero ETL integresion - AUrora -> Redshit 

Reshfit Streaming INgestion

Usages of Commands : 
COPY command is typically used to move data from s3 to Redshift 
if the data is already in Redshit : INSERT into select * 
create table as select * 

Copy can decrypt data from s3
automatic compression option 

Narrow table : many rows few columns
load it with single copy command 
hidden metadata columns consume too much space
DBLINK 
sync data between postgres sql and resshift


Concurrency scaling : automatically scale clusters to handle bursts loads
WLM queues manage which queries are sent to concuurrency scaling clusters


short query Acceleration :
prioritize short running queries over long running onces 
they run in dedicated space wont wati for long onces
works with create table as 
and select statements
can configure how many seconds is short 

VACCUM command :
Recover space from deleted rows
VACCUM FULL 
VACCUM DELETE ONE 
VACCUM SORT ONLY 
VACCUM REINDEX

Redshit anti patterns :(do not use rds for these)
small data sets
OLTP 
Unstrcutured data 
BLOB data (store them in s3)

Resizing Redshift Clusters :

Elastic Resize : 
quickly add or remove nodes of same type 
Classic Resize : 
change node type and number of node
read only

Latest features :
RA3 nodes : 
enable independect scaling of compute and storage

Red shift data lake export :

unload redshift query to s3 in parquet
automatically partitioned 

Spatial data types :
Geometry,geography

Cross region data sharing :
share live data across redhsit clusters without copying a
it required ra3 nodes

Redshift Server less : 

Auto scale based on load
Monitoring 
SYS_QUERY_HISTORY
SYS_LOAD_HISTORY
SYS_SERVERLESS_USAGE

CLoud watch logs 
Cloud watch metrics

MAterialized Views : 
Autorefresh 
can run once a day 

Redshfit data Sharing :

Securely share live data across multiple clusters 
why ?
1. workload isolaaiton 
2. cross group collaboration 
3. sharing data between env
can share db,tables,udfs, views 

REdshfit Lambda UDF:
use lamba functions in SQL queries 
need to grant usage on this fuctnion

Redshift Federated Queries : 
Query and analuze across databases,warehouses and lakes

Redshift System tables and Views:
SYS views
STV tables 
SVV views
STL logs 
SVCS details about queries on main and concurrency scaling clusters
SVL views : details aboiut quries on main clusters 

Redshift Data API :
secured HTTP endpiint for SQL stement to Redshift Clusters 
Asyncronous 
integrate with other AWS functionality (AWS LAMBDA,Event Bridge)

MAximum parameters : 
Query Duration : 24 hours 
Active Queries : 500
query result size (gxiped):100MB
Result retention time : 24 hours
query statment size: 100kb
packet for query data per row: 64KB
30 transactions per statement









